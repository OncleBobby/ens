{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdea75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, warnings, time, numpy, yaml\n",
    "sys.path.append(\"../src/\") # go to parent dir\n",
    "from main import get_X, get_y, get_score, get_train_test\n",
    "# from models import get_model_benchmark1, get_model_benchmark2, show_importance\n",
    "from models.model import Model\n",
    "from models.factory import ModelFactory\n",
    "from models.benchmark import Benchmark1, Benchmark2\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f766bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_X('train')\n",
    "train_scores = get_y()\n",
    "test_data = get_X('test')\n",
    "X_train, y_train, X_test, y_test, X_valid, y_valid, target = get_train_test(train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282620c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "benchmark1 = Benchmark1(X_train, y_train, X_valid, y_valid, train_scores)\n",
    "benchmark1.train()\n",
    "benchmark1.evaluate(X_test)\n",
    "# benchmark1.save(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "benchmark2 = Benchmark2(X_train, y_train, X_valid, y_valid, train_scores)\n",
    "benchmark2.train()\n",
    "benchmark2.evaluate(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark2.save(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_by_name={\n",
    "    'benchmark2': {\n",
    "          'booster': 'gbtree',\n",
    "          'tree_method':'hist',\n",
    "          'max_depth': 8, \n",
    "          'learning_rate': 0.025,\n",
    "          'objective': 'multi:softprob',\n",
    "          'num_class': 2,\n",
    "          'eval_metric':'mlogloss'\n",
    "        },\n",
    "    'benchmark3': {\n",
    "          'booster': 'gblinear',\n",
    "          'tree_method':'hist',\n",
    "          'max_depth': 8, \n",
    "          'learning_rate': 0.025,\n",
    "          'objective': 'multi:softprob',\n",
    "          'num_class': 2,\n",
    "          'eval_metric':'mlogloss'\n",
    "        },\n",
    "}\n",
    "for name, params in params_by_name.items():\n",
    "    benchmark2 = Benchmark2(X_train, y_train, X_valid, y_valid, train_scores, params)\n",
    "    benchmark2.train()\n",
    "    benchmark2.evaluate(X_test)\n",
    "    benchmark2.name = name\n",
    "    benchmark2.save(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "def get_params(booster, tree_method, eval_metric='mlogloss'):\n",
    "    return {\n",
    "          'booster': booster,\n",
    "          'tree_method': tree_method,\n",
    "          'max_depth': 8, \n",
    "          'learning_rate': 0.025,\n",
    "          'objective': 'multi:softprob',\n",
    "          'num_class': 2,\n",
    "          'eval_metric':eval_metric\n",
    "        }\n",
    "# boosters = ['gbtree', 'gblinear', 'dart']\n",
    "boosters = ['gblinear']\n",
    "# tree_methods = ['auto', 'exact', 'approx', 'hist']\n",
    "tree_methods = ['hist']\n",
    "# eval_metrics = ['mphe', 'merror', 'mlogloss', 'auc']\n",
    "eval_metrics = ['mlogloss']\n",
    "for booster in boosters:\n",
    "    for tree_method in tree_methods:\n",
    "        for eval_metric in eval_metrics:\n",
    "          start = time.time()\n",
    "          name = f'{booster}_{tree_method}_{eval_metric}'\n",
    "          params = get_params(booster, tree_method, eval_metric)\n",
    "          benchmark2 = Benchmark2(X_train, y_train, X_valid, y_valid, train_scores, params)\n",
    "          benchmark2.train()\n",
    "          score = benchmark2.evaluate(X_test)\n",
    "          end = time.time()\n",
    "          print(f'{name}={score} in {numpy.round((end-start), 2)}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0045d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import import_module\n",
    "\n",
    "class_str: str = 'models.benchmark.Benchmark2'\n",
    "try:\n",
    "    module_path, class_name = class_str.rsplit('.', 1)\n",
    "    module = import_module(module_path)\n",
    "    model = getattr(module, class_name)(X_train, y_train, X_valid, y_valid, train_scores, params)\n",
    "    benchmark2.train()\n",
    "    print(benchmark2.evaluate(X_test))\n",
    "except (ImportError, AttributeError) as e:\n",
    "    raise ImportError(class_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'benchmark3'\n",
    "with open('../confs/models.yaml', 'r') as file:\n",
    "    configurations = yaml.safe_load(file)\n",
    "factory = ModelFactory(configurations, X_train, y_train, X_valid, y_valid, train_scores)\n",
    "model = factory.get_model(name)\n",
    "model.train()\n",
    "print(model.evaluate(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
